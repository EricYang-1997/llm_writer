# -*- coding: utf-8 -*-
import asyncio
import os
import re
from typing import Any, Dict, Generator, List, Literal
from neo4j import GraphDatabase  # Áî®‰∫éÊ∏ÖÂ∫ì & Ê£ÄÊü•ËäÇÁÇπÊï∞
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.openai_like import OpenAILikeEmbedding
# === ÊõøÊç¢ÂØºÂÖ• ===
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.core import Document, Settings, StorageContext
from llama_index.core.indices.property_graph import PropertyGraphIndex, SchemaLLMPathExtractor
from llama_index.vector_stores.milvus import MilvusVectorStore
from llama_index.vector_stores.milvus.utils import BM25BuiltInFunction
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import SimpleDirectoryReader
import llama_index.core
from llama_index.core.tools import RetrieverTool
from llama_index.core.agent.workflow import ReActAgent
from llama_index.core.chat_engine.types import ChatMessage

from graph import read_docx_to_graph_nodes

# llama_index.core.set_global_handler("simple")

os.environ["OPENAI_API_KEY"] = "dummy"  # Èò≤Ê≠¢‰ªª‰Ωï OpenAI ÂõûÈÄÄ
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

# ===================== ÈÖçÁΩÆÂå∫ÔºàÊåâÈúÄ‰øÆÊîπÔºâ =====================

DASHSCOPE_API_KEY = "sk-a8ca287e30304c23803c3910fffc76d2"
API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# 1) ÈÖçÁΩÆÁôæÁÇº LLMÔºåÂπ∂ÁªëÂÆö‰∏∫ÂÖ®Â±Ä
llm = OpenAILike(
    model="qwen-plus",
    api_base=API_BASE,
    api_key=DASHSCOPE_API_KEY,
    is_chat_model=True,
    timeout=300,
)

embed_model = OpenAILikeEmbedding(
    model_name="text-embedding-v4",
    api_base=API_BASE,
    api_key=DASHSCOPE_API_KEY,
    dimensions=1536
)



# === ÊõøÊç¢‰∏∫ Neo4jPropertyGraphStore ===
graph_store = Neo4jPropertyGraphStore(
    username="neo4j",          # ‚Üê Ê†πÊçÆ‰Ω†ÁöÑ Neo4j ËÆæÁΩÆ‰øÆÊîπ
    password="ygy1997666",  # ‚Üê ÈªòËÆ§È¶ñÊ¨°ÁôªÂΩïÂêéÂøÖÈ°ª‰øÆÊîπ
    url="bolt://127.0.0.1:7687",  # Neo4j ÈªòËÆ§ bolt Á´ØÂè£ÊòØ 7687Ôºà‰∏çÊòØ 7688Ôºâ
    database="neo4j",          # ÈªòËÆ§Êï∞ÊçÆÂ∫ìÂêç
)

REBUILD_GRAPH = False   

if REBUILD_GRAPH:
    storage_context = StorageContext.from_defaults(property_graph_store=graph_store)
else:
    storage_context = StorageContext.from_defaults(property_graph_store=graph_store, persist_dir="./storage")
    llm = Ollama(model="qwen3:8b", base_url="http://192.168.2.122:11434",request_timeout=300,thinking=False) 

Settings.llm = llm  
Settings.embed_model = embed_model  

# ===================== ÊûÑÂª∫ÂõæË∞±ÔºàÂê´Ê∏ÖÂ∫ìÔºâ=====================

def build_graph_index():
    # Ê∏ÖÁ©∫Êï∞ÊçÆÂ∫ìÔºàNeo4j ÂÖºÂÆπÔºâ
    driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "ygy1997666"))
    with driver.session() as session:
        session.run("MATCH (n) DETACH DELETE n;")
        session.run("""
            CALL apoc.schema.assert({},{},true) YIELD label, key
            RETURN *
        """)
    driver.close()

    extractor = SchemaLLMPathExtractor(
        llm=llm,
        max_triplets_per_chunk=3,
        num_workers=10,
        strict=False  
    )
    nodes = read_docx_to_graph_nodes("./docs/test.docx", graph_store)

    index = PropertyGraphIndex(
        nodes=nodes,
        llm=llm,
        use_async=False,
        embed_model=embed_model,
        kg_extractors=[extractor],
        property_graph_store=graph_store,
        storage_context=storage_context,
        show_progress=True,
        embed_kg_nodes=True,
    )

    nodes = index.property_graph_store.get()
    storage_context.persist(persist_dir="./storage")
    graph_store.persist(persist_path="./storage/property_graph_store.json")

    for node in nodes:
        print(node)
    print(f"üéØ ÂõæË∞±ÂÖ± {len(nodes)} ‰∏™ËäÇÁÇπ:")
    return index   

# ===================== Âä†ËΩΩÂ∑≤ÊúâÂõæË∞± =====================
def load_existing_graph_index():
    print("‚úÖ ÂõæË∞±Á¥¢ÂºïÂä†ËΩΩÊàêÂäüÔºÅ")
    storage_context.persist(persist_dir="./storage")
    graph_store.persist(persist_path="./storage/property_graph_store.json")
    index = PropertyGraphIndex.from_existing(
        llm=llm,
        embed_model=embed_model,
        property_graph_store=graph_store,
        storage_context=storage_context,
        use_async=False,
        embed_kg_nodes=True,
    )
    nodes = index.property_graph_store.get()
    print(f"üéØ ÂõæË∞±ÂÖ± {len(nodes)} ‰∏™ËäÇÁÇπ:")
    return index

# ===================== Âæ™ÁéØÂØπËØù =====================

def create_agent():
    """
    Ê†πÊçÆÊü•ËØ¢ÊñáÊú¨‰ªéÂõæÁ¥¢Âºï‰∏≠Ëé∑ÂèñÂçïÊ¨°ÂìçÂ∫î„ÄÇ
    """
    # ËÆæÁΩÆÂÖ®Â±ÄÊ®°Âûã
    Settings.embed_model = embed_model  
    Settings.llm = llm
    index = load_existing_graph_index()
    # ÂàõÂª∫ÂõæÊÑüÁü•Ê£ÄÁ¥¢Âô®
    retriever = index.as_retriever(
        include_text=True,
        include_graph=True,
        similarity_top_k=10,
        graph_traversal_depth=10,
    )

    # Â∞ÜÊ£ÄÁ¥¢Âô®ÂåÖË£Ö‰∏∫Â∑•ÂÖ∑
    retriever_tool = RetrieverTool.from_defaults(
        retriever=retriever,
        name="graph_retriever",
        description="Useful for retrieving information from the knowledge graph."
    )
    # ÂàõÂª∫ FunctionAgent
    agent = ReActAgent(
        tools=[retriever_tool],  # ‰ΩøÁî®Â∑•ÂÖ∑ÂàóË°®
        llm=llm
    )
    
    return agent


class LlamaIndexChatWrapper:
    def __init__(self, index):
        self.index = index
        # Ê≥®ÊÑèÔºöllm ÈúÄË¶Å‰ªéÂ§ñÈÉ®‰º†ÂÖ•ÊàñÂú®ÊñπÊ≥ï‰∏≠Ëé∑Âèñ
        # self.llm = llm  # ÂÅáËÆæ llm Â∑≤ÂÆö‰πâ

    def chat(self, messages: List[Dict[str, str]], stream: bool = False):
        if not messages:
            raise ValueError("messages ‰∏çËÉΩ‰∏∫Á©∫")

        # ÊãÜÂàÜÔºöÊúÄÂêé‰∏ÄÊù°ÊòØÁî®Êà∑ÂΩìÂâçÈóÆÈ¢òÔºåÂâçÈù¢ÁöÑÊòØÂéÜÂè≤
        if len(messages) == 1:
            chat_history = []
            current_query = messages[0]["content"]
        else:
            chat_history = [
                ChatMessage(role=msg["role"], content=msg["content"])
                for msg in messages[:-1]
            ]
            current_query = messages[-1]["content"]

        if stream:
            return self._stream_response(current_query, chat_history)
        else:
            return self._sync_response(current_query, chat_history)

    def _sync_response(self, query: str, chat_history: List[ChatMessage]) -> str:
        """ÂêåÊ≠•ÂìçÂ∫î"""
        chat_engine = self.index.as_chat_engine(
            # llm=self.llm,  # Â¶ÇÊûúÈúÄË¶ÅÔºåÂèñÊ∂àÊ≥®Èáä
            similarity_top_k=10,
            graph_traversal_depth=10,
            chat_history=chat_history,
        )
        response = chat_engine.chat(query)
        return str(response)

    def _stream_response(self, query: str, chat_history: List[ChatMessage]) -> Generator[str, None, None]:
        if not query.strip():
            yield "Êä±Ê≠âÔºåÊàëÊ≤°Êî∂Âà∞ÊúâÊïàÁöÑÈóÆÈ¢ò„ÄÇËØ∑ÈáçÊñ∞ÊèêÈóÆ„ÄÇ"
            return

        chat_engine = self.index.as_chat_engine(
            chat_history=chat_history,
            streaming=True
        )
        response_gen = chat_engine.stream_chat(query)

        for token in response_gen.response_gen:
            if token:  # Âè™ÊúâÈùûÁ©∫tokenÊâçËæìÂá∫
                yield token

        yield " [DONE]"

# ‰ΩøÁî®Á§∫‰æã
def create_chat_completion(messages: List[Dict[str, str]], stream: bool = True):
    """
    Â∞ÅË£ÖÂáΩÊï∞ÔºåÊ®°Êãü client.chat.completions.create
    """
    wrapper = LlamaIndexChatWrapper(index=load_existing_graph_index())
    
    if stream:
        return wrapper.chat(messages, stream=True)
    else:
        return wrapper.chat(messages, stream=False)

def test():
    """ÊµãËØïÂáΩÊï∞"""
    # Á¨¨‰∏ÄËΩÆÂØπËØù
    test_messages_1 = [
        {"role": "user", "content": "‰Ω†Â•Ω"}
    ]
    
    print("Á¨¨‰∏ÄËΩÆ - ÊµãËØïÊµÅÂºèËæìÂá∫:")
    stream_gen = create_chat_completion(test_messages_1, stream=True)
    full_response_1 = ""
    for chunk in stream_gen:
        print(f"ÊµÅÂºèÂùó: {chunk}", end="")
        full_response_1 += chunk if chunk not in [" [DONE]\n\n"] else ""
    print("\n" + "="*50)
    
    # Á¨¨‰∫åËΩÆÂØπËØùÔºàÂ∏¶‰∏ä‰∏ãÊñáÔºâ
    test_messages_2 = [
        {"role": "user", "content": "‰Ω†Â•Ω"}, 
        {"role": "assistant", "content": full_response_1.strip()},  # ‰∏äËΩÆÂõûÂ§ç
        {"role": "user", "content": "ÂàöÊâçÊàëÈóÆ‰∫Ü‰ªÄ‰πàÔºü"}  # ÂΩìÂâçÈóÆÈ¢ò
    ]
    
    print("Á¨¨‰∫åËΩÆ - ÊµãËØï‰∏ä‰∏ãÊñá:")
    sync_response_2 = create_chat_completion(test_messages_2, stream=False)
    print(f"Â∏¶‰∏ä‰∏ãÊñáÁöÑÂìçÂ∫î: {sync_response_2}")
    print("="*50)
    
    # È™åËØÅ‰∏ä‰∏ãÊñáÊòØÂê¶ÁîüÊïà
    print("È™åËØÅÔºöÊ®°ÂûãÂ∫îËÉΩÂõûÁ≠î'ÂàöÊâçÊàëÈóÆ‰∫Ü‰ªÄ‰πà' -> '‰Ω†Â•Ω'")

# ===================== ‰∏ªÁ®ãÂ∫è =====================
if __name__ == "__main__":
    if REBUILD_GRAPH:
        build_graph_index()

    else:
        test()
